{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUgRbcMjHFq+A8oeUhKBp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshan-stack/Auto-R/blob/main/Multithreading_concept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i00e0wwU1t-1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "\n",
        "# just some colors to make output easier to read\n",
        "class Colors:\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    ORANGE = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    END = '\\033[0m'\n",
        "    BOLD = '\\033[1m'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulating LLM Gateway Operations\n",
        "\n",
        "Testing out different scenarios:\n",
        "- Slow LLM API Call (GPT-4 style) - about 3 seconds\n",
        "- Fast Cache Hit (Redis) - 0.5 seconds\n",
        "- Database Query (budget check) - 1 second"
      ],
      "metadata": {
        "id": "-LF6pmJC1xwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def customer_a_llm_request(customer_id):\n",
        "    # simulating a slow GPT-4 call\n",
        "    start = time.time()\n",
        "    print(f\"{Colors.BLUE} Customer A: Starting GPT-4 API call...{Colors.END}\")\n",
        "\n",
        "    time.sleep(3)  # API is slow\n",
        "\n",
        "    end = time.time()\n",
        "    result = f\"GPT-4 Response: 'Hello, I can help you!'\"\n",
        "    print(f\"{Colors.BLUE} Customer A: Got response! Took {end-start:.2f}s{Colors.END}\")\n",
        "    return result\n",
        "\n",
        "def customer_b_cache_hit(customer_id):\n",
        "    # this should be fast - just reading from cache\n",
        "    start = time.time()\n",
        "    print(f\"{Colors.GREEN} Customer B: Checking cache...{Colors.END}\")\n",
        "\n",
        "    time.sleep(0.5)  # cache lookup\n",
        "\n",
        "    end = time.time()\n",
        "    result = f\"Cached Response: 'AI is Artificial Intelligence'\"\n",
        "    print(f\"{Colors.GREEN} Customer B: Cache hit! {end-start:.2f}s{Colors.END}\")\n",
        "    return result\n",
        "\n",
        "def customer_c_budget_check(customer_id):\n",
        "    # checking database for budget\n",
        "    start = time.time()\n",
        "    print(f\"{Colors.ORANGE} Customer C: Running budget query...{Colors.END}\")\n",
        "\n",
        "    time.sleep(1)  # db query\n",
        "\n",
        "    end = time.time()\n",
        "    result = f\"Budget OK: User has $50 remaining\"\n",
        "    print(f\"{Colors.ORANGE} Customer C: Done. {end-start:.2f}s{Colors.END}\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "9F7zZYA22IeS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1: Single-threaded approach\n",
        "\n",
        "The problem here: everything runs one after another\n",
        "- Customer B has to wait for A to finish\n",
        "- Customer C waits for both A and B\n",
        "- Total time adds up"
      ],
      "metadata": {
        "id": "TO8M2n2-2YHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}SINGLE-THREADED (sequential processing){Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\\n\")\n",
        "\n",
        "start_total = time.time()\n",
        "\n",
        "# just running them one by one\n",
        "result_a = customer_a_llm_request(\"A\")\n",
        "result_b = customer_b_cache_hit(\"B\")\n",
        "result_c = customer_c_budget_check(\"C\")\n",
        "\n",
        "end_total = time.time()\n",
        "total_time = end_total - start_total\n",
        "\n",
        "print(f\"\\n{Colors.RED}{Colors.BOLD}Total time: {total_time:.2f} seconds{Colors.END}\")\n",
        "print(f\"{Colors.RED}Customer B waited {total_time:.2f}s even though their request only needed 0.5s{Colors.END}\")\n",
        "print(f\"{Colors.RED}Customer C also waited the full {total_time:.2f}s{Colors.END}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9vqAr7O2bST",
        "outputId": "753ac3e1-1293-4dd0-c52d-52d7801d7609"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================================================\u001b[0m\n",
            "\u001b[1mSINGLE-THREADED (sequential processing)\u001b[0m\n",
            "\u001b[1m============================================================\u001b[0m\n",
            "\n",
            "\u001b[94m Customer A: Starting GPT-4 API call...\u001b[0m\n",
            "\u001b[94m Customer A: Got response! Took 3.00s\u001b[0m\n",
            "\u001b[92m Customer B: Checking cache...\u001b[0m\n",
            "\u001b[92m Customer B: Cache hit! 0.50s\u001b[0m\n",
            "\u001b[93m Customer C: Running budget query...\u001b[0m\n",
            "\u001b[93m Customer C: Done. 1.00s\u001b[0m\n",
            "\n",
            "\u001b[91m\u001b[1mTotal time: 4.50 seconds\u001b[0m\n",
            "\u001b[91mCustomer B waited 4.50s even though their request only needed 0.5s\u001b[0m\n",
            "\u001b[91mCustomer C also waited the full 4.50s\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Multi-threaded\n",
        "\n",
        "Now let's try running everything at the same time\n",
        "- All customers get served simultaneously\n",
        "- Total time should be roughly the slowest operation\n",
        "- Much better!"
      ],
      "metadata": {
        "id": "ZG_PuiK02kxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}MULTI-THREADED (parallel processing){Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\\n\")\n",
        "\n",
        "start_total = time.time()\n",
        "\n",
        "# using ThreadPoolExecutor to run them all at once\n",
        "with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "    # submit all tasks\n",
        "    future_a = executor.submit(customer_a_llm_request, \"A\")\n",
        "    future_b = executor.submit(customer_b_cache_hit, \"B\")\n",
        "    future_c = executor.submit(customer_c_budget_check, \"C\")\n",
        "\n",
        "    # get results\n",
        "    result_a = future_a.result()\n",
        "    result_b = future_b.result()\n",
        "    result_c = future_c.result()\n",
        "\n",
        "end_total = time.time()\n",
        "total_time = end_total - start_total\n",
        "\n",
        "print(f\"\\n{Colors.GREEN}{Colors.BOLD}Total time: {total_time:.2f} seconds{Colors.END}\")\n",
        "print(f\"{Colors.GREEN}All customers served in {total_time:.2f}s (just the slowest operation){Colors.END}\")\n",
        "print(f\"{Colors.GREEN}Customer B finished in 0.5s and didn't wait{Colors.END}\")\n",
        "print(f\"{Colors.GREEN}Customer C finished in 1s and didn't wait{Colors.END}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQxV0lAZ2sIU",
        "outputId": "d6656335-6cdc-4601-ff11-5a1994e7d7a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================================================\u001b[0m\n",
            "\u001b[1mMULTI-THREADED (parallel processing)\u001b[0m\n",
            "\u001b[1m============================================================\u001b[0m\n",
            "\n",
            "\u001b[94m Customer A: Starting GPT-4 API call...\u001b[0m\n",
            "\u001b[92m Customer B: Checking cache...\u001b[0m\n",
            "\u001b[93m Customer C: Running budget query...\u001b[0m\n",
            "\u001b[92m Customer B: Cache hit! 0.50s\u001b[0m\n",
            "\u001b[93m Customer C: Done. 1.00s\u001b[0m\n",
            "\u001b[94m Customer A: Got response! Took 3.00s\u001b[0m\n",
            "\n",
            "\u001b[92m\u001b[1mTotal time: 3.00 seconds\u001b[0m\n",
            "\u001b[92mAll customers served in 3.00s (just the slowest operation)\u001b[0m\n",
            "\u001b[92mCustomer B finished in 0.5s and didn't wait\u001b[0m\n",
            "\u001b[92mCustomer C finished in 1s and didn't wait\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance\n",
        "\n"
      ],
      "metadata": {
        "id": "SaKXmEss2w9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's measure both approaches properly\n",
        "import time\n",
        "\n",
        "# single threaded - everything adds up\n",
        "start = time.time()\n",
        "time.sleep(3)  # LLM call\n",
        "time.sleep(0.5)  # Cache hit\n",
        "time.sleep(1)  # Budget check\n",
        "single_threaded_time = time.time() - start\n",
        "\n",
        "# multi threaded - just the slowest one matters\n",
        "multi_threaded_time = max(3, 0.5, 1)\n",
        "\n",
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}COMPARISON{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\\n\")\n",
        "\n",
        "print(f\"Single-threaded: {single_threaded_time:.2f}s\")\n",
        "print(f\"Multi-threaded:  {multi_threaded_time:.2f}s\")\n",
        "print(f\"\\n{Colors.GREEN}Speed improvement: {(single_threaded_time/multi_threaded_time):.2f}x faster{Colors.END}\")\n",
        "print(f\"{Colors.GREEN}Can handle {(single_threaded_time/multi_threaded_time):.2f}x more customers{Colors.END}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dWwQrJj24mw",
        "outputId": "4774efa5-987a-4799-cf48-9dbbeca1aa36"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================================================\u001b[0m\n",
            "\u001b[1mCOMPARISON\u001b[0m\n",
            "\u001b[1m============================================================\u001b[0m\n",
            "\n",
            "Single-threaded: 4.50s\n",
            "Multi-threaded:  3.00s\n",
            "\n",
            "\u001b[92mSpeed improvement: 1.50x faster\u001b[0m\n",
            "\u001b[92mCan handle 1.50x more customers\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real scenario - handling 10 customers\n",
        "\n",
        "Let's try something more realistic with 10 different requests:\n",
        "- 4 slow LLM calls\n",
        "- 4 fast cache hits\n",
        "- 2 budget checks\n"
      ],
      "metadata": {
        "id": "5yF9DTy53A8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_request(request_id, request_type):\n",
        "    # handle different request types\n",
        "    start = time.time()\n",
        "\n",
        "    if request_type == \"llm\":\n",
        "        print(f\" Request {request_id}: Calling LLM API...\")\n",
        "        time.sleep(3)\n",
        "        result = \"LLM Response\"\n",
        "    elif request_type == \"cache\":\n",
        "        print(f\" Request {request_id}: Cache Hit!\")\n",
        "        time.sleep(0.5)\n",
        "        result = \"Cached Data\"\n",
        "    else:  # budget check\n",
        "        print(f\" Request {request_id}: Budget Check...\")\n",
        "        time.sleep(1)\n",
        "        result = \"Budget OK\"\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\" Request {request_id} done in {end-start:.2f}s\")\n",
        "    return result\n",
        "\n",
        "# 10 different customer requests\n",
        "requests = [\n",
        "    (1, \"llm\"), (2, \"cache\"), (3, \"llm\"), (4, \"cache\"),\n",
        "    (5, \"budget\"), (6, \"llm\"), (7, \"cache\"), (8, \"llm\"),\n",
        "    (9, \"cache\"), (10, \"budget\")\n",
        "]"
      ],
      "metadata": {
        "id": "Eaq7R1Gh3HMJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-threaded: everyone gets served at once"
      ],
      "metadata": {
        "id": "yQenSQUH3aUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}MULTI-THREADED: all 10 customers at once{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{'='*60}{Colors.END}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = [executor.submit(process_request, req_id, req_type)\n",
        "               for req_id, req_type in requests]\n",
        "\n",
        "    results = [future.result() for future in futures]\n",
        "\n",
        "multi_total = time.time() - start_time\n",
        "print(f\"\\n{Colors.GREEN}{Colors.BOLD}Total time: {multi_total:.2f} seconds{Colors.END}\")\n",
        "print(f\"{Colors.GREEN}All 10 customers done in {multi_total:.2f}s!{Colors.END}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqarFNbT3di7",
        "outputId": "3c729b72-66ef-4ae2-a097-a1af57d6145e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================================================\u001b[0m\n",
            "\u001b[1mMULTI-THREADED: all 10 customers at once\u001b[0m\n",
            "\u001b[1m============================================================\u001b[0m\n",
            "\n",
            " Request 1: Calling LLM API...\n",
            " Request 2: Cache Hit!\n",
            " Request 3: Calling LLM API...\n",
            " Request 4: Cache Hit!\n",
            " Request 5: Budget Check...\n",
            " Request 6: Calling LLM API...\n",
            " Request 7: Cache Hit!\n",
            " Request 8: Calling LLM API...\n",
            " Request 9: Cache Hit!\n",
            " Request 10: Budget Check...\n",
            " Request 2 done in 0.50s\n",
            " Request 4 done in 0.50s\n",
            " Request 7 done in 0.50s\n",
            " Request 9 done in 0.50s\n",
            " Request 5 done in 1.00s\n",
            " Request 10 done in 1.00s\n",
            " Request 1 done in 3.00s\n",
            " Request 3 done in 3.00s\n",
            " Request 6 done in 3.00s\n",
            " Request 8 done in 3.00s\n",
            "\n",
            "\u001b[92m\u001b[1mTotal time: 3.01 seconds\u001b[0m\n",
            "\u001b[92mAll 10 customers done in 3.01s!\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final comparison\n"
      ],
      "metadata": {
        "id": "klztDH3f3qbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{Colors.BOLD}{'='*70}{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}          MULTITHREADING RESULTS          {Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{'='*70}{Colors.END}\\n\")\n",
        "\n",
        "print(f\"Scenario: 10 customers (4 LLM, 4 cache, 2 DB)\\n\")\n",
        "\n",
        "# calculating expected times\n",
        "single_expected = (4 * 3) + (4 * 0.5) + (2 * 1)  # everything adds up\n",
        "multi_expected = 3  # just the slowest one\n",
        "\n",
        "print(f\"{Colors.RED}Single-threaded:{Colors.END}\")\n",
        "print(f\"   Time: {single_expected:.1f}s\")\n",
        "print(f\"   (everyone waits in line)\\n\")\n",
        "\n",
        "print(f\"{Colors.GREEN}Multi-threaded:{Colors.END}\")\n",
        "print(f\"   Time: ~{multi_expected:.1f}s\")\n",
        "print(f\"   (all processed simultaneously)\\n\")\n",
        "\n",
        "speedup = single_expected / multi_expected\n",
        "print(f\"{Colors.BOLD}{Colors.GREEN}Performance: {speedup:.1f}x faster{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{Colors.GREEN}Throughput: {speedup:.1f}x more customers/sec{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{Colors.GREEN}Much better user experience\\n{Colors.END}\")\n",
        "\n",
        "print(f\"{Colors.BOLD}{'='*70}{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}This is basically how FastAPI handles requests{Colors.END}\")\n",
        "print(f\"{Colors.BOLD}{'='*70}{Colors.END}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUAIuEf53tAm",
        "outputId": "7160b334-38c8-46b9-f331-73697c2b8258"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================================================\u001b[0m\n",
            "\u001b[1m          MULTITHREADING RESULTS          \u001b[0m\n",
            "\u001b[1m======================================================================\u001b[0m\n",
            "\n",
            "Scenario: 10 customers (4 LLM, 4 cache, 2 DB)\n",
            "\n",
            "\u001b[91mSingle-threaded:\u001b[0m\n",
            "   Time: 16.0s\n",
            "   (everyone waits in line)\n",
            "\n",
            "\u001b[92mMulti-threaded:\u001b[0m\n",
            "   Time: ~3.0s\n",
            "   (all processed simultaneously)\n",
            "\n",
            "\u001b[1m\u001b[92mPerformance: 5.3x faster\u001b[0m\n",
            "\u001b[1m\u001b[92mThroughput: 5.3x more customers/sec\u001b[0m\n",
            "\u001b[1m\u001b[92mMuch better user experience\n",
            "\u001b[0m\n",
            "\u001b[1m======================================================================\u001b[0m\n",
            "\u001b[1mThis is basically how FastAPI handles requests\u001b[0m\n",
            "\u001b[1m======================================================================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}